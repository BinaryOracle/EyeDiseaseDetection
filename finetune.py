import argparse
import os
import warnings
import time
import numpy as np
from pathlib import Path
from collections import Counter

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
import models_vit as models
from timm.models.layers import trunc_normal_
from config import device

warnings.simplefilter(action='ignore', category=FutureWarning)

# å›¾åƒé¢„å¤„ç†
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Resize(255),
    transforms.CenterCrop(255),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

test_transform = transforms.Compose([
    transforms.Resize(255),
    transforms.CenterCrop(255),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

class_tabel = ('A', 'C', 'CSC', 'D', 'G', 'N', 'RP' , 'RVO')
class_tabel = ('A', 'C', 'CSC', 'D0','D1','D2','D3','D4', 'G', 'N', 'RP' , 'BRVO','CRVO')

IMG_EXTS = {'.jpg', '.jpeg', '.png', '.tiff', '.TIFF'}

# ------------------ Focal Loss ------------------
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        logpt = -nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(logpt)
        loss = -((1 - pt) ** self.gamma) * logpt
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class FundusDataset(Dataset):
    def __init__(self, img_paths, labels, transform=None):
        self.img_paths = img_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        label = self.labels[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, label

def load_data(args):
    all_img_paths, all_labels = [], []

    root = Path(args.data_path)

    for i, class_name in enumerate(class_tabel):
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ç›®å½•åä¸º class_name çš„ç›®å½•
        for class_dir in root.rglob(class_name):
            if class_dir.is_dir():
                # åœ¨è¯¥ç›®å½•ä¸‹é€’å½’æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
                for img_path in class_dir.rglob('*'):
                    if img_path.suffix.lower() in IMG_EXTS and img_path.is_file():
                        all_img_paths.append(str(img_path))
                        all_labels.append(i)

    # åˆ†æç±»åˆ«åˆ†å¸ƒ
    label_counts = Counter(all_labels)
    print("ğŸ“Š åŸå§‹æ•°æ®ç±»åˆ«åˆ†å¸ƒ:")
    for i, class_name in enumerate(class_tabel):
        print(f"  {class_name}: {label_counts[i]} å¼ å›¾ç‰‡")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = compute_class_weight(
        'balanced', 
        classes=np.unique(all_labels), 
        y=all_labels
    )
    class_weights = torch.FloatTensor(class_weights)
    print(f"âš–ï¸  è®¡ç®—å¾—åˆ°çš„ç±»åˆ«æƒé‡: {class_weights.numpy()}")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
    train_img_paths, val_img_paths, train_labels, val_labels = train_test_split(
        all_img_paths, all_labels, 
        test_size=args.val_ratio, 
        random_state=42,
        stratify=all_labels  # åˆ†å±‚é‡‡æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹
    )

    # æ„å»ºæ•°æ®é›†
    train_dataset = FundusDataset(train_img_paths, train_labels, transform=train_transform)
    val_dataset = FundusDataset(val_img_paths, val_labels, transform=test_transform)

    # ä¸ºè®­ç»ƒé›†åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
    train_label_counts = Counter(train_labels)
    train_sample_weights = [1.0 / train_label_counts[label] for label in train_labels]
    train_sampler = WeightedRandomSampler(
        weights=train_sample_weights,
        num_samples=len(train_labels),
        replacement=True
    )

    # æ„å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        sampler=train_sampler,  # ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨
        num_workers=4
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=4
    )

    return train_loader, val_loader, class_weights

def load_model(args, device):
    if args.model == 'RETFound_mae':
        model = models.__dict__[args.model](
            img_size=args.input_size,
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            global_pool=args.global_pool,
        )
    else:
        model = models.__dict__[args.model](
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            args=args,
        )

    checkpoint_path = args.models_path
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    checkpoint_model = checkpoint['model']

    checkpoint_model = {k.replace("backbone.", ""): v for k, v in checkpoint_model.items()}
    checkpoint_model = {k.replace("mlp.w12.", "mlp.fc1."): v for k, v in checkpoint_model.items()}
    checkpoint_model = {k.replace("mlp.w3.", "mlp.fc2."): v for k, v in checkpoint_model.items()}

    state_dict = model.state_dict()
    for k in ['head.weight', 'head.bias']:
        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:
            print(f"Removing key {k} from pretrained checkpoint")
            del checkpoint_model[k]

    # load pre-trained model
    model.load_state_dict(checkpoint_model, strict=False)

    trunc_normal_(model.head.weight, std=2e-5)

    # åˆå§‹åŒ–åˆ†ç±»å¤´æƒé‡
    if hasattr(model, 'head') and isinstance(model.head, nn.Linear):
        trunc_normal_(model.head.weight, std=2e-5)
        if model.head.bias is not None:
            nn.init.constant_(model.head.bias, 0)

    model.to(device, dtype=torch.float32)

    # å†»ç»“é™¤åˆ†ç±»å¤´å¤–çš„æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        param.requires_grad = name.startswith("head")

    return model

def load_finetuned_model(args, device):
    if args.model == 'RETFound_mae':
        model = models.__dict__[args.model](
            img_size=args.input_size,
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            global_pool=args.global_pool,
        )
    else:
        model = models.__dict__[args.model](
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            args=args,
        )

    # åŠ è½½ä¿å­˜çš„å¾®è°ƒæƒé‡
    checkpoint_path = os.path.join(args.save_path, 'best_model.pth')
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¾®è°ƒæƒé‡æ–‡ä»¶: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # å°è¯•åŠ è½½æƒé‡ï¼Œè®°å½•ä¸åŒ¹é…é¡¹
    model.load_state_dict(checkpoint, strict=False)

    model.to(device, dtype=torch.float32)

    # å†»ç»“é™¤åˆ†ç±»å¤´å’Œ Adapter å¤–çš„æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        if name.startswith("head") or "adapter" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    return model

def load_model_finetuned(args, device):
    if args.model == 'RETFound_mae':
        model = models.__dict__[args.model](
            img_size=args.input_size,
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            global_pool=args.global_pool,
        )
    else:
        model = models.__dict__[args.model](
            num_classes=args.nb_classes,
            drop_path_rate=args.drop_path,
            args=args,
        )

    # åŠ è½½ä¿å­˜çš„å¾®è°ƒæƒé‡
    checkpoint_path = args.models_path
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¾®è°ƒæƒé‡æ–‡ä»¶: {checkpoint_path}")

    # åŠ è½½ checkpointï¼Œä¸åŠ è½½ head
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    state_dict = checkpoint.copy()
    for k in list(state_dict.keys()):
        if k.startswith('head.'):
            del state_dict[k]

    model.load_state_dict(state_dict, strict=False)
    model.to(device, dtype=torch.float32)

    # é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
    if hasattr(model, 'head') and isinstance(model.head, nn.Linear):
        trunc_normal_(model.head.weight, std=2e-5)
        if model.head.bias is not None:
            nn.init.constant_(model.head.bias, 0)

    # å†»ç»“é™¤åˆ†ç±»å¤´å’Œ Adapter å¤–çš„æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        if name.startswith("head") or "adapter" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    return model

def calculate_class_metrics(outputs, labels, num_classes):
    """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡"""
    # ç¡®ä¿outputså’Œlabelséƒ½æ˜¯æ­£ç¡®çš„å½¢çŠ¶
    if outputs.dim() == 1:
        predicted = outputs  # å¦‚æœoutputså·²ç»æ˜¯é¢„æµ‹ç»“æœ
    else:
        _, predicted = outputs.max(1)  # å¦‚æœoutputsæ˜¯æ¨¡å‹è¾“å‡º
    
    correct = predicted.eq(labels)
    
    class_correct = torch.zeros(num_classes)
    class_total = torch.zeros(num_classes)
    
    for i in range(num_classes):
        mask = (labels == i)
        if mask.sum() > 0:
            class_correct[i] = correct[mask].sum()
        class_total[i] = mask.sum()
    
    return class_correct, class_total

def train(model, train_loader, val_loader, args, device, class_weights):
    # ç¼©æ”¾åˆ° [0.5, 2.0]
    min_alpha, max_alpha = 0.5, 2.0
    alpha = min_alpha + (class_weights - class_weights.min()) / (class_weights.max() - class_weights.min()) * (
                max_alpha - min_alpha)
    alpha = alpha.to(device)
    # ä½¿ç”¨ Focal Lossï¼Œç»“åˆç±»åˆ«æƒé‡ alpha ä¿è¯å¤§ç±»åˆ«ä¸è¢«å¿½ç•¥
    criterion = FocalLoss(alpha=alpha, gamma=1.5)
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=0.05)
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=args.lr * 0.01
    )
    
    best_val_acc = 85
    best_val_f1 = 0
    prev_train_loss = None  # åˆå§‹åŒ–prev_train_loss

    # è®¡ç®—æ€»è¿­ä»£æ¬¡æ•°
    total_iterations = len(train_loader) * args.epochs
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»è½®æ•°: {args.epochs}, æ¯è½®è¿­ä»£æ•°: {len(train_loader)}, æ€»è¿­ä»£æ•°: {total_iterations}")
    print(f"ğŸ“Š å­¦ä¹ ç‡: {args.lr}, æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"âš–ï¸  ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡")
    print("=" * 80)

    for epoch in range(args.epochs):
        start_time = time.time()
        model.train()
        train_loss = 0
        
        print(f"ğŸ”„ Epoch {epoch+1}/{args.epochs} å¼€å§‹è®­ç»ƒ...")

        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device, dtype=torch.float32), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
            
            # æ¯10ä¸ªæ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡è¿›åº¦å’ŒæŸå¤±
            if (batch_idx + 1) % 10 == 0:
                avg_loss = train_loss / (batch_idx + 1)
                progress = (epoch * len(train_loader) + batch_idx + 1) / (len(train_loader) * args.epochs) * 100
                print(f"  ğŸ“ Epoch {epoch+1}/{args.epochs} | Batch {batch_idx+1}/{len(train_loader)} | "
                      f"Progress: {progress:.1f}% | Current Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f}")

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = train_loss / len(train_loader)
        
        # è¯„ä¼°é˜¶æ®µ
        print(f"ğŸ” Epoch {epoch+1}/{args.epochs} å¼€å§‹éªŒè¯...")
        model.eval()
        val_loss, correct, total = 0, 0, 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels_batch in val_loader:
                images, labels_batch = images.to(device, dtype=torch.float32), labels_batch.to(device)
                outputs = model(images)
                val_loss += criterion(outputs, labels_batch).item()
                _, predicted = outputs.max(1)
                total += labels_batch.size(0)
                correct += predicted.eq(labels_batch).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels_batch.cpu().numpy())

        val_acc = 100. * correct / total
        avg_val_loss = val_loss / len(val_loader)
        elapsed = time.time() - start_time
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_correct, class_total = calculate_class_metrics(
            torch.tensor(all_predictions), 
            torch.tensor(all_labels), 
            args.nb_classes
        )
        
        # è®¡ç®—æ€»ä½“è¿›åº¦
        overall_progress = (epoch + 1) / args.epochs * 100

        print(f"ğŸ“ˆ Epoch {epoch+1}/{args.epochs} å®Œæˆ | æ€»ä½“è¿›åº¦: {overall_progress:.1f}%")
        print(f"   ğŸ¯ è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        print(f"   â±ï¸  è€—æ—¶: {elapsed:.2f}s | æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print("   ğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i, class_name in enumerate(class_tabel):
            if class_total[i] > 0:
                class_acc = 100. * class_correct[i] / class_total[i]
                print(f"     {class_name}: {class_acc:.1f}% ({int(class_correct[i])}/{int(class_total[i])})")
        
        # æŸå¤±å˜åŒ–è¶‹åŠ¿
        if epoch > 0 and prev_train_loss is not None:
            loss_change = avg_train_loss - prev_train_loss
            loss_trend = "â†—ï¸" if loss_change > 0 else "â†˜ï¸" if loss_change < 0 else "â¡ï¸"
            print(f"   ğŸ“Š æŸå¤±å˜åŒ–: {loss_change:+.4f} {loss_trend}")
        
        prev_train_loss = avg_train_loss
        print("-" * 80)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        print(f"   ğŸ“š å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            os.makedirs(args.save_path, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(args.save_path, 'checkpoint-best.pth'))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {best_val_acc:.2f}%")
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.join(args.save_path, 'checkpoint-best.pth')}")
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

def convert_path_to_unix_style(path):
    if os.path.sep == '\\':
        return path
    return path.replace('\\', '/')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--models_path', type=str, required=True)
    parser.add_argument('--data_path', type=str, required=True)
    parser.add_argument('--model', type=str, required=True)
    parser.add_argument('--input_size', default=255, type=int)
    parser.add_argument('--nb_classes', default=13, type=int)
    parser.add_argument('--global_pool', default='token')
    parser.add_argument('--drop_path', type=float, default=0.2)
    parser.add_argument('--val_ratio', type=float, default=0.2)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--save_path', type=str, required=True)
    args = parser.parse_args()

    # å…¼å®¹ Unix è·¯å¾„
    if os.name == 'posix':
        args.models_path = convert_path_to_unix_style(args.models_path)
        args.data_path = convert_path_to_unix_style(args.data_path)
        args.save_path = convert_path_to_unix_style(args.save_path)

    model = load_model_finetuned(args, device)
    train_loader, val_loader, class_weights = load_data(args)
    train(model, train_loader, val_loader, args, device, class_weights)